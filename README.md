# RAG System Project

A robust Retrieval-Augmented Generation (RAG) system built with FastAPI that enables document upload, intelligent processing, vector-based similarity search, and AI-powered answer generation. Upload files, automatically process them into searchable chunks with embeddings, store in PostgreSQL with pgvector and Qdrant vector database, and retrieve contextual answers powered by LLMs for your AI applications.

[![Python](https://img.shields.io/badge/Python-3.12%2B-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.118.0-009688.svg)](https://fastapi.tiangolo.com/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-18.0-336791.svg)](https://www.postgresql.org/)
[![pgvector](https://img.shields.io/badge/pgvector-0.8.1-orange.svg)](https://github.com/pgvector/pgvector)
[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](LICENSE)

## ğŸ—ï¸ Architecture Overview

### Core Components

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client App    â”‚â”€â”€â”€â”€â–¶â”‚   FastAPI API   â”‚â”€â”€â”€â”€â–¶â”‚   Controllers   â”‚
â”‚  (Upload/Query) â”‚     â”‚   Routes        â”‚     â”‚  (Business      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   Logic)        â”‚
                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                          â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â–¼                                 â–¼               â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  PostgreSQL   â”‚              â”‚ LLM Providersâ”‚  â”‚ VectorDB     â”‚
                â”‚  + pgvector   â”‚              â”‚ (OpenAI,     â”‚  â”‚ (PGVector/   â”‚
                â”‚  (Chunks,     â”‚              â”‚  Cohere)     â”‚  â”‚  Qdrant)     â”‚
                â”‚   Projects,   â”‚              â”‚              â”‚  â”‚              â”‚
                â”‚   Assets)     â”‚              â”‚              â”‚  â”‚              â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â–²                              â–²                  â–²
                        â”‚                              â”‚                  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   LangChain     â”‚â”€â”€â”€â”€â–¶â”‚   Document      â”‚
                        â”‚  Text Splitter  â”‚     â”‚   Loaders       â”‚
                        â”‚  (Chunking)     â”‚     â”‚  (PDF, TXT)     â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â–²
                                   â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   File Storage  â”‚
                        â”‚  (Project-based â”‚
                        â”‚   Organization) â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **Document Upload** â†’ File validation â†’ Unique naming â†’ Project storage
2. **Document Processing** â†’ Content extraction â†’ Text chunking â†’ Metadata preservation
3. **Data Storage** â†’ PostgreSQL (via SQLAlchemy async) â†’ Project organization â†’ Asset tracking
4. **Vector Embeddings** â†’ LLM Provider (Cohere/OpenAI) â†’ Generate embeddings â†’ Store in VectorDB (PGVector or Qdrant)
5. **Similarity Search** â†’ Query vectors â†’ VectorDB search â†’ Retrieve top-k relevant chunks
6. **Answer Generation** â†’ Prompt construction with context â†’ LLM generation â†’ AI-powered answers

### Provider Architecture

The system uses a **Factory Pattern** for extensible provider management:

**LLM Providers:**

- Abstract `LLMInterface` defines the contract
- `LLMProviderFactory` creates provider instances
- Support for OpenAI and Cohere (easily extensible)
- Unified API for text generation and embeddings
- Multi-language prompt templates with dynamic imports

**VectorDB Providers:**

- Abstract `VectorDBInterface` defines the contract
- `VectorDBProviderFactory` creates provider instances
- **PGVector** implementation for PostgreSQL with pgvector extension
- **Qdrant** implementation for standalone vector storage
- Support for collection management and similarity search
- Configurable distance metrics (cosine, dot product, L2)

## ğŸ› ï¸ Technical Stack

- **Backend Framework**: FastAPI with async/await patterns and lifespan context management
- **Database**: PostgreSQL 18.0 with pgvector extension (v0.8.1) for vector similarity
- **ORM**: SQLAlchemy 2.0 with async support (asyncpg driver)
- **Database Migrations**: Alembic for schema version control
- **Vector Database**: PGVector (PostgreSQL) or Qdrant for vector storage and similarity search
- **LLM Providers**: OpenAI and Cohere with factory pattern (supports custom OpenAI-compatible APIs)
- **Template Engine**: Multi-language prompt template system with Python string.Template
- **Document Processing**: LangChain (text splitting, document loading)
- **PDF Processing**: PyMuPDF (FitzPDF) for efficient PDF extraction
- **Data Validation**: Pydantic v2 with custom validators
- **File Handling**: aiofiles for async I/O operations
- **Task Queue**: Celery with Redis for background processing
- **Monitoring**: Prometheus metrics with starlette-exporter
- **Containerization**: Docker & Docker Compose
- **Python Version**: 3.12+ / 3.13
- **Additional Libraries**: asyncpg, sqlalchemy, alembic, aiofiles, python-dotenv, python-multipart, qdrant-client, openai, cohere, langchain, motor, redis, flower

## ğŸ“ Project Structure

```text
src/
â”œâ”€â”€ main.py                          # FastAPI application & lifespan context
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ helper/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ config.py                    # Application settings management
â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                      # Health/version endpoints
â”‚   â”œâ”€â”€ data_route.py                # File upload & processing endpoints
â”‚   â”œâ”€â”€ nlp.py                       # RAG endpoints (push, search, generate)
â”‚   â””â”€â”€ schemas/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ dataproces_schemas.py    # Request/response schemas
â”‚       â””â”€â”€ nlp.py                   # NLP/RAG schemas
â”œâ”€â”€ controllers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ BaseContoller.py             # Base controller functionality
â”‚   â”œâ”€â”€ DataController.py            # File validation & storage
â”‚   â”œâ”€â”€ ProcessController.py         # Document processing & chunking
â”‚   â””â”€â”€ NLPController.py             # RAG logic (search, answer generation)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ BaseDataModel.py             # Base async SQLAlchemy model
â”‚   â”œâ”€â”€ ChunkModel.py                # DataChunk DAL (async)
â”‚   â”œâ”€â”€ ProjectModel.py              # Project DAL (async)
â”‚   â”œâ”€â”€ AssetModel.py                # Asset DAL (async)
â”‚   â”œâ”€â”€ db_schemas/
â”‚   â”‚   â”œâ”€â”€ __init__.py              # Public schema exports
â”‚   â”‚   â”œâ”€â”€ rag/                     # RAG database schemas
â”‚   â”‚   â”‚   â”œâ”€â”€ alembic.ini          # Alembic configuration
â”‚   â”‚   â”‚   â”œâ”€â”€ alembic/             # Migration scripts
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ env.py           # Migration environment
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ versions/        # Migration versions
â”‚   â”‚   â”‚   â””â”€â”€ schemas/
â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚       â”œâ”€â”€ rag_base.py      # SQLAlchemy Base
â”‚   â”‚   â”‚       â”œâ”€â”€ chunks_schemas.py # DataChunk model
â”‚   â”‚   â”‚       â”œâ”€â”€ project_shemas.py # Project model
â”‚   â”‚   â”‚       â””â”€â”€ asset.py         # Asset model
â”‚   â”œâ”€â”€ enums/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ProcesseEnums.py         # Document type enums
â”‚   â”‚   â”œâ”€â”€ ResponseEnums.py         # API response enums
â”‚   â”‚   â””â”€â”€ __pycache__/
â”‚   â””â”€â”€ __pycache__/
â”œâ”€â”€ stores/                          # External service providers
â”‚   â”œâ”€â”€ llm/                         # LLM providers (OpenAI, Cohere)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ LLMInterface.py          # Abstract LLM interface
â”‚   â”‚   â”œâ”€â”€ LLMEnums.py              # LLM provider enums
â”‚   â”‚   â”œâ”€â”€ LLMProviderFactory.py    # Factory for LLM providers
â”‚   â”‚   â”œâ”€â”€ providers/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ OpenAIProvider.py    # OpenAI implementation
â”‚   â”‚   â”‚   â””â”€â”€ CoHereProvider.py    # Cohere implementation
â”‚   â”‚   â””â”€â”€ templete/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ templete_parser.py   # Template parser for prompts
â”‚   â”‚       â””â”€â”€ locales/
â”‚   â”‚           â”œâ”€â”€ ar/              # Arabic templates
â”‚   â”‚           â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚           â”‚   â””â”€â”€ rag.py
â”‚   â”‚           â””â”€â”€ en/              # English templates
â”‚   â”‚               â”œâ”€â”€ __init__.py
â”‚   â”‚               â””â”€â”€ rag.py
â”‚   â””â”€â”€ vectordb/                    # Vector database providers
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ VectorDBInterface.py     # Abstract VectorDB interface
â”‚       â”œâ”€â”€ VectorDBEnums.py         # VectorDB provider enums
â”‚       â”œâ”€â”€ VectorDBProviderFactory.py # Factory for VectorDB providers
â”‚       â””â”€â”€ providers/
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ QdrantDBProvider.py  # Qdrant implementation
â”‚           â””â”€â”€ PGVectorProvider.py  # PostgreSQL pgvector implementation
â””â”€â”€ assets/
    â””â”€â”€ files/                       # File storage (organized by project)
        â””â”€â”€ {project_id}/            # Project-specific directories

docker/
â”œâ”€â”€ docker-compose.yml               # PostgreSQL + pgvector service
â”œâ”€â”€ .env                             # Database credentials (not committed)
â”œâ”€â”€ DATABASE_CONNECTIONS.md          # Connection guide for DBeaver/pgAdmin
â””â”€â”€ QUICK_REFERENCE.txt              # Quick reference card

.gitignore                          # Root gitignore
README.md                           # This file
LICENSE                            # Project license
```

## ğŸš€ API Endpoints

### Base Endpoints

- `GET /api/v1/` - Application information and health check health

### Data Management Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/v1/data/upload/{project_id}` | Upload files to a project (returns asset_id) |
| `POST` | `/api/v1/data/processall/{project_id}` | Process all files in project, save chunks to PostgreSQL |
| `POST` | `/api/v1/data/processone/{project_id}` | Process single file, save chunks with optional reset |

### NLP/RAG Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/v1/nlp/push` | Push documents to vector database with embeddings |
| `POST` | `/api/v1/nlp/search` | Search for similar documents using vector similarity |
| `POST` | `/api/v1/nlp/generate` | Generate AI-powered answers based on query and context |

### Response Structure

All endpoints return JSON responses with status indicators:

```json
{
  "status": "success_code",
  "message": "Descriptive message",
  "data": {}
}
```

### Request/Response Examples

**Upload File:**

```bash
curl -X POST "http://localhost:8000/api/v1/data/upload/my_project" \
     -F "file=@document.pdf"

# Response:
{
  "status": "file_upload_success",
  "file_path": "/path/to/file",
  "file_id": "unique_filename",
  "asset_id": "507f1f77bcf86cd799439011"
}
```

**Process Single File:**

```bash
curl -X POST "http://localhost:8000/api/v1/data/processone/my_project" \
     -H "Content-Type: application/json" \
     -d '{
       "file_id": "abc123_document.pdf",
       "chunk_size": 1000,
       "overlap_size": 100,
       "do_reset": false
     }'

# Response:
{
  "status": "processing_success",
  "total_chunks": 42,
  "inserted_chunks": 42,
  "chunks": [...]
}
```

**Process All Files:**

```bash
curl -X POST "http://localhost:8000/api/v1/data/processall/my_project"

# Response:
{
  "status": "processing_success",
  "total_files": 5,
  "processed_files": 5,
  "failed_files": 0,
  "total_chunks": 150,
  "inserted_chunks": 150
}
```

**Push Documents to Vector Database:**

```bash
curl -X POST "http://localhost:8000/api/v1/nlp/push" \
     -H "Content-Type: application/json" \
     -d '{
       "project_id": "my_project",
       "do_reset": false
     }'

# Response:
{
  "status": "success",
  "message": "Successfully pushed 150 chunks to vector database",
  "total_chunks": 150,
  "embedding_model": "embed-v4.0",
  "vector_dimension": 256
}
```

**Search Similar Documents:**

```bash
curl -X POST "http://localhost:8000/api/v1/nlp/search" \
     -H "Content-Type: application/json" \
     -d '{
       "project_id": "my_project",
       "query": "What is the main topic?",
       "top_k": 5
     }'

# Response:
{
  "status": "success",
  "results": [
    {
      "chunk_text": "The main topic discusses...",
      "score": 0.89,
      "chunk_id": "507f1f77bcf86cd799439011"
    },
    ...
  ],
  "total_results": 5
}
```

**Generate AI-Powered Answer:**

```bash
curl -X POST "http://localhost:8000/api/v1/nlp/generate" \
     -H "Content-Type: application/json" \
     -d '{
       "project_id": "my_project",
       "query": "What is the main topic?",
       "language": "en",
       "top_k": 5
     }'

# Response:
{
  "status": "success",
  "answer": "Based on the documents, the main topic discusses...",
  "context_documents_count": 5
}
```

## ğŸ”§ Configuration

### RAG Workflow

The RAG system follows a complete pipeline from document upload to AI-powered answer generation:

1. **Upload Documents**: Upload PDF or text files to project-specific directories
2. **Process & Chunk**: Extract text and split into semantic chunks with overlap
3. **Generate Embeddings**: Create vector embeddings using Cohere or OpenAI
4. **Store Vectors**: Index embeddings in vector database (PGVector or Qdrant) for similarity search
5. **Query Processing**: Convert user queries into embeddings
6. **Retrieve Context**: Find top-k most relevant document chunks via vector similarity
7. **Prompt Construction**: Build context-aware prompts with multi-language templates
8. **Generate Answers**: Use LLM to generate answers based on retrieved context

### Key Features

- **Multi-Provider Support**: Switch between OpenAI and Cohere for embeddings and generation
- **Custom LLM Endpoints**: Use OpenAI-compatible APIs (e.g., local Ollama models via ngrok)
- **Vector Search**: Similarity search with configurable distance metrics (cosine, dot product)
- **Template System**: Multi-language prompt templates with dynamic variable substitution
- **Async Processing**: Non-blocking I/O for efficient file processing and database operations
- **Lazy Loading**: Optimized startup with on-demand provider initialization
- **Flexible Chunking**: Configurable chunk sizes and overlap for optimal retrieval
- **Project Isolation**: Separate vector collections per project for organization

### Environment Variables

Create a `.env` file in the `src/` directory with the following variables (see `src/.env.example` for a template):


## ğŸ“‹ Prerequisites & Installation

### Prerequisites

- Python 3.12+ (or 3.13)
- Docker & Docker Compose
- Git
- PostgreSQL client (optional, for direct database access)

### Quick Start

1. **Clone the repository:**

   ```bash
   git clone https://github.com/SaeedNeamtallah/AI-Customer-Copilot.git
   cd AI-Customer-Copilot
   ```

2. **Create and activate virtual environment:**

   ```bash
   # On Linux/Mac
   python3 -m venv venv
   source venv/bin/activate

   # On Windows
   python -m venv venv
   venv\Scripts\activate
   ```

3. **Install dependencies:**

   ```bash
   cd src
   pip install -r requirements.txt
   ```

4. **Configure environment variables:**

   ```bash
   # Create .env file in src/ directory
   cp .env.example .env
   # Edit .env with your API keys and database credentials
   ```

5. **Start PostgreSQL with Docker Compose:**

   ```bash
   cd ../docker
   # Create .env file for Docker (see docker/.env.example)
   docker-compose up -d
   ```

6. **Run database migrations:**

   ```bash
   cd ../src/models/db_schemas/rag
   alembic upgrade head
   ```

7. **Run the application:**

   ```bash
   cd ../../../
   uvicorn main:app --reload --host 0.0.0.0 --port 8000
   ```

8. **Access the API:**
   - API Documentation: `http://localhost:8000/docs`
   - ReDoc Documentation: `http://localhost:8000/redoc`
   - API Base URL: `http://localhost:8000/api/v1`

## ğŸ“Š Database Schema

### PostgreSQL Tables (with pgvector extension)

#### `projects` Table

```sql
CREATE TABLE projects (
    project_id SERIAL PRIMARY KEY,
    project_uuid UUID UNIQUE NOT NULL DEFAULT gen_random_uuid(),
    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE
);
```

#### `assets` Table

```sql
CREATE TABLE assets (
    asset_id SERIAL PRIMARY KEY,
    asset_uuid UUID UNIQUE NOT NULL DEFAULT gen_random_uuid(),
    asset_project_id INTEGER NOT NULL REFERENCES projects(project_id),
    asset_type VARCHAR(50) NOT NULL,
    asset_name VARCHAR(255) NOT NULL,
    asset_size INTEGER,
    asset_config JSONB,
    asset_pushed_at TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE,
    UNIQUE (asset_project_id, asset_name)
);
```

#### `chunks` Table

```sql
CREATE TABLE chunks (
    chunk_id SERIAL PRIMARY KEY,
    chunk_uuid UUID UNIQUE NOT NULL DEFAULT gen_random_uuid(),
    chunk_text TEXT NOT NULL,
    chunk_metadata JSONB,
    chunk_order INTEGER NOT NULL,
    chunk_project_id INTEGER NOT NULL REFERENCES projects(project_id),
    chunk_asset_id INTEGER NOT NULL REFERENCES assets(asset_id),
    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE
);

CREATE INDEX ix_chunk_project_id ON chunks(chunk_project_id);
CREATE INDEX ix_chunk_asset_id ON chunks(chunk_asset_id);
```

### Schema Features

- **UUID Support**: All tables have UUID fields for external references
- **JSONB**: Flexible metadata storage for chunks and asset configurations
- **Foreign Keys**: Proper relationships between projects, assets, and chunks
- **Timestamps**: Automatic tracking of creation and update times
- **Indexes**: Optimized for common query patterns

## ğŸ§ª Testing

### Manual Testing - Complete RAG Workflow

```bash
# 1. Upload a PDF file
curl -X POST "http://localhost:8000/api/v1/data/upload/test_project" \
     -F "file=@sample.pdf"

# 2. Process the uploaded file into chunks
curl -X POST "http://localhost:8000/api/v1/data/processone/test_project" \
     -H "Content-Type: application/json" \
     -d '{
       "file_id": "abc123_sample.pdf",
       "chunk_size": 1000,
       "overlap_size": 100,
       "do_reset": false
     }'

# 3. Push chunks to vector database with embeddings
curl -X POST "http://localhost:8000/api/v1/nlp/push" \
     -H "Content-Type: application/json" \
     -d '{
       "project_id": "test_project",
       "do_reset": false
     }'

# 4. Search for similar documents
curl -X POST "http://localhost:8000/api/v1/nlp/search" \
     -H "Content-Type: application/json" \
     -d '{
       "project_id": "test_project",
       "query": "What is the main topic?",
       "top_k": 5
     }'

# 5. Generate AI-powered answer
curl -X POST "http://localhost:8000/api/v1/nlp/generate" \
     -H "Content-Type: application/json" \
     -d '{
       "project_id": "test_project",
       "query": "What is the main topic?",
       "language": "en",
       "top_k": 5
     }'

# 6. Verify data in PostgreSQL
# Option 1: Using psql
docker exec -it vector-postgres psql -U postgres -d ai_vectors
# SELECT * FROM projects WHERE project_id = 3;
# SELECT COUNT(*) FROM chunks WHERE chunk_project_id = 3;

# Option 2: Using DBeaver (see docker/DATABASE_CONNECTIONS.md)
```

## ğŸ“ License

This project is licensed under the Apache License 2.0. See [LICENSE](LICENSE) file for details.

## ğŸ‘¥ Contributors

- Saeed Neamtallah (@SaeedNeamtallah)
